# Job Aggregator - File Structure

## Project Overview
A no-backend job aggregator that scrapes "hidden" software engineering roles from multiple sources and displays them in a static web frontend.

## Directory Structure

```
Job Aggregator/
â”‚
â”œâ”€â”€ ğŸ“ scrapers/                    # Web scraper modules
â”‚   â”œâ”€â”€ __init__.py                 # Package initialization
â”‚   â”œâ”€â”€ hn_scraper.py               # HackerNews Who's Hiring scraper
â”‚   â”œâ”€â”€ yc_scraper.py               # Y Combinator Jobs scraper
â”‚   â”œâ”€â”€ wellfound_scraper.py        # Wellfound (AngelList) scraper
â”‚   â”œâ”€â”€ remoteok_scraper.py         # RemoteOK scraper
â”‚   â””â”€â”€ weworkremotely_scraper.py   # We Work Remotely scraper
â”‚
â”œâ”€â”€ ğŸ“ utils/                        # Utility functions
â”‚   â”œâ”€â”€ __init__.py                 # Package initialization
â”‚   â”œâ”€â”€ hidden_score.py             # Calculates "hidden score" (0-100)
â”‚   â””â”€â”€ deduplication.py            # Removes duplicate job postings
â”‚
â”œâ”€â”€ ğŸ“ frontend/                     # Static web frontend
â”‚   â”œâ”€â”€ index.html                  # Main HTML page
â”‚   â”œâ”€â”€ style.css                   # Stylesheet
â”‚   â”œâ”€â”€ app.js                      # JavaScript for filtering/searching
â”‚   â”œâ”€â”€ jobs.json                   # Generated job data (updated by scraper)
â”‚   â””â”€â”€ .nojekyll                   # Tells GitHub Pages to skip Jekyll
â”‚
â”œâ”€â”€ ğŸ“ data/                        # Historical scraped data (optional)
â”‚   â””â”€â”€ hn_jobs_*.json              # Timestamped backup files
â”‚
â”œâ”€â”€ ğŸ“ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ scrape.yml             # GitHub Actions workflow (daily auto-scrape)
â”‚
â”œâ”€â”€ ğŸ“„ scraper.py                   # Main entry point - orchestrates all scrapers
â”œâ”€â”€ ğŸ“„ models.py                    # JobPosting data model definition
â”œâ”€â”€ ğŸ“„ config.json                  # Configuration for scraper sources
â”œâ”€â”€ ğŸ“„ jobs.json                    # Unified output file (all jobs combined)
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ“„ README.md                    # Project documentation
â”œâ”€â”€ ğŸ“„ .gitignore                   # Git ignore rules
â”œâ”€â”€ ğŸ“„ vercel.json                  # Vercel deployment config
â””â”€â”€ ğŸ“„ _config.yml                  # GitHub Pages config (optional)
```

## File Descriptions

### Core Python Files

**`scraper.py`** - Main orchestrator
- Imports all scrapers
- Runs scraping from config.json
- Applies deduplication
- Calculates hidden scores
- Saves unified `jobs.json` output

**`models.py`** - Data model
- Defines `JobPosting` dataclass
- Fields: company, title, location, tech_stack, url, source, posted_date, hidden_score
- Methods: `generate_id()`, `to_dict()`, `from_dict()`

**`config.json`** - Source configuration
- Lists all job sources (HN threads, YC, Wellfound, etc.)
- `active: true/false` toggles scraping for each source
- Add new sources here

### Scraper Modules (`scrapers/`)

Each scraper follows the same pattern:
- `scrape_jobs()` or `scrape_thread()` method
- Extracts: company, title, location, tech_stack, url, posted_date
- Returns list of `JobPosting` objects

**`hn_scraper.py`** - HN Who's Hiring
- Parses HN comment threads
- Extracts job info from unstructured comments
- Handles comment replies and filtering

**`yc_scraper.py`** - Y Combinator Jobs
- Scrapes YC jobs board
- Parses structured job listings

**`wellfound_scraper.py`** - Wellfound/AngelList
- Scrapes Wellfound job board
- May need JavaScript rendering for full functionality

**`remoteok_scraper.py`** - RemoteOK
- Scrapes RemoteOK job board
- Focuses on remote positions

**`weworkremotely_scraper.py`** - We Work Remotely
- Scrapes We Work Remotely programming jobs
- Remote-focused positions

### Utility Modules (`utils/`)

**`hidden_score.py`** - Scoring algorithm
- Source weights: HN (90), YC (80), Wellfound (70), others (20)
- Recency bonus: 24hrs (+10), 1 week (+5), 2 weeks (0)
- Returns score 0-100

**`deduplication.py`** - Duplicate removal
- Uses fuzzy matching (rapidfuzz library)
- Compares company + title similarity
- Prevents duplicate job postings

### Frontend (`frontend/`)

**`index.html`** - Main page structure
- Search input
- Location dropdown
- Tech stack filter tags
- Job cards container

**`app.js`** - Frontend logic
- Fetches `jobs.json` on load
- Client-side filtering (search, location, tech stack)
- Renders job cards
- Sorts by hidden_score

**`style.css`** - Styling
- Modern, responsive design
- Card-based layout
- Gradient headers
- Mobile-friendly

**`jobs.json`** - Generated data file
- Created by `scraper.py`
- Unified format with all jobs
- Sorted by hidden_score (descending)
- Updated daily via GitHub Actions

### Configuration Files

**`requirements.txt`** - Python dependencies
- beautifulsoup4 (HTML parsing)
- requests (HTTP requests)
- lxml (XML/HTML parser)
- python-dateutil (date parsing)
- rapidfuzz (fuzzy string matching)

**`config.json`** - Scraper configuration
- Defines which sources to scrape
- Lists HN thread URLs
- Can enable/disable sources

**`.github/workflows/scrape.yml`** - Automation
- Runs daily at 2 AM UTC
- Installs dependencies
- Runs scraper
- Commits updated jobs.json
- Can be triggered manually

**`vercel.json`** - Vercel deployment
- Configures static file serving
- Routes requests to frontend/

**`_config.yml`** - GitHub Pages config
- Optional Jekyll configuration

### Output Files

**`jobs.json`** (root) - Main output
- All scraped jobs in unified format
- Used by frontend

**`frontend/jobs.json`** - Copy for frontend
- Same as root jobs.json
- Makes it accessible to frontend

**`data/hn_jobs_*.json`** - Historical backups
- Timestamped files from previous runs
- Useful for debugging/testing

## Data Flow

```
1. scraper.py reads config.json
2. Runs each active scraper (HN, YC, etc.)
3. Each scraper returns JobPosting objects
4. scraper.py:
   - Calculates hidden_score for each job
   - Deduplicates using fuzzy matching
   - Sorts by hidden_score
5. Saves to jobs.json (root + frontend/)
6. Frontend loads jobs.json
7. User filters/searches client-side
8. GitHub Actions runs daily to update jobs.json
```

## Key Design Decisions

1. **No Backend**: Everything is static files - jobs.json + frontend
2. **Unified Output**: Single jobs.json file with consistent schema
3. **Client-Side Filtering**: No server needed, all filtering in browser
4. **Daily Updates**: GitHub Actions runs scrapers automatically
5. **Modular Scrapers**: Each source has its own scraper module
6. **Deduplication**: Prevents same job appearing multiple times
7. **Hidden Score**: Ranks jobs by "hidden-ness" (source + recency)

## How to Add a New Scraper

1. Create `scrapers/new_scraper.py`
2. Implement `scrape_jobs()` method returning `JobPosting` objects
3. Add to `scrapers/__init__.py` exports
4. Update `scraper.py` to call new scraper
5. Add source config to `config.json`
6. Add source weight to `utils/hidden_score.py`

## Deployment Files

- **GitHub Pages**: Uses `frontend/` directory, `.nojekyll` file
- **Vercel**: Uses `vercel.json` config
- **GitHub Actions**: Automatically updates jobs.json daily

